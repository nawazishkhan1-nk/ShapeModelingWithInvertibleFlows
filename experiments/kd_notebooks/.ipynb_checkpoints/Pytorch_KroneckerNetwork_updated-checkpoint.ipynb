{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a408ec5",
   "metadata": {
    "id": "e7cb1b7c"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b8b99ed",
   "metadata": {
    "id": "65bc0136"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform,target_transform=transforms.Compose([\n",
    "                                 lambda x:torch.tensor([x]), # or just torch.tensor\n",
    "                                 lambda x:F.one_hot(x,10)]),)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a0bf8c17",
   "metadata": {
    "id": "4e83ff8a"
   },
   "outputs": [],
   "source": [
    "# class KDRightLayer(nn.Module):\n",
    "#     \"\"\" Custom KDRightLayer\"\"\"\n",
    "#     def __init__(self, n, q, m, activation_string='Identity'):\n",
    "#         super().__init__()\n",
    "#         self.n = n\n",
    "#         self.activation_string = activation_string\n",
    "#         self.activation = getattr(nn, self.activation_string)()  \n",
    "\n",
    "        \n",
    "#         self.q = q #input_shape[1]\n",
    "#         self.m = m #input_shape[2]\n",
    "        \n",
    "#         weights = torch.Tensor(m, self.n)\n",
    "#         self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "#         bias = torch.Tensor(self.q, self.n )\n",
    "#         self.bias = nn.Parameter(bias)\n",
    "\n",
    "#         # initialize weights and biases\n",
    "#         nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "#         fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "#         bound = 1 / math.sqrt(fan_in)\n",
    "#         nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return torch.matmul(input, self.weights) + self.bias\n",
    "\n",
    "class KDLeftLayer(nn.Module):\n",
    "    \"\"\" Custom KDRightLayer\"\"\"\n",
    "    def __init__(self, input_dim, weight_dim, activation_string='Identity'):\n",
    "        super().__init__()\n",
    "        self.m = input_dim[0]\n",
    "        self.n = input_dim[1]\n",
    "        self.activation_string = activation_string\n",
    "        self.activation =  getattr(nn, self.activation_string)()  \n",
    "\n",
    "        \n",
    "        weights = torch.Tensor(weight_dim[0], weight_dim[1])\n",
    "        self.weights = nn.Parameter(weights)\n",
    "        \n",
    "        bias = torch.Tensor(input_dim[0], weight_dim[1])\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        \n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.matmul(input, self.weights) + self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "70b41401",
   "metadata": {
    "id": "0626f288"
   },
   "outputs": [],
   "source": [
    "class KDRightLayer(nn.Module):\n",
    "    \"\"\" Custom KDLeftLayer\"\"\"\n",
    "    def __init__(self, input_dim, weight_dim, activation_string='Identity'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation_string = activation_string\n",
    "        self.activation =  getattr(nn, self.activation_string)()  \n",
    "\n",
    "        \n",
    "        weights = torch.Tensor(weight_dim[0], weight_dim[1])\n",
    "        self.weights = nn.Parameter(weights)\n",
    "        \n",
    "        bias = torch.Tensor(weight_dim[0], input_dim[1])\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        \n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(\"custom_left\")\n",
    "        return torch.permute(torch.matmul(torch.permute(input,dims=[0, 2, 1]),\n",
    "                                                  self.weights.t()),dims=[0, 2, 1]) + self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e45a17d7",
   "metadata": {
    "id": "c53c1a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─KDLeftLayer: 1-1                       --\n",
      "|    └─ReLU: 2-1                         --\n",
      "├─KDRightLayer: 1-2                      --\n",
      "|    └─ReLU: 2-2                         --\n",
      "├─KDLeftLayer: 1-3                       --\n",
      "|    └─ReLU: 2-3                         --\n",
      "├─KDRightLayer: 1-4                      --\n",
      "|    └─ReLU: 2-4                         --\n",
      "├─KDLeftLayer: 1-5                       --\n",
      "|    └─ReLU: 2-5                         --\n",
      "├─KDRightLayer: 1-6                      --\n",
      "|    └─ReLU: 2-6                         --\n",
      "├─KDLeftLayer: 1-7                       --\n",
      "|    └─ReLU: 2-7                         --\n",
      "├─KDRightLayer: 1-8                      --\n",
      "|    └─ReLU: 2-8                         --\n",
      "├─Flatten: 1-9                           --\n",
      "├─LogSoftmax: 1-10                       --\n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─KDLeftLayer: 1-1                       --\n",
       "|    └─ReLU: 2-1                         --\n",
       "├─KDRightLayer: 1-2                      --\n",
       "|    └─ReLU: 2-2                         --\n",
       "├─KDLeftLayer: 1-3                       --\n",
       "|    └─ReLU: 2-3                         --\n",
       "├─KDRightLayer: 1-4                      --\n",
       "|    └─ReLU: 2-4                         --\n",
       "├─KDLeftLayer: 1-5                       --\n",
       "|    └─ReLU: 2-5                         --\n",
       "├─KDRightLayer: 1-6                      --\n",
       "|    └─ReLU: 2-6                         --\n",
       "├─KDLeftLayer: 1-7                       --\n",
       "|    └─ReLU: 2-7                         --\n",
       "├─KDRightLayer: 1-8                      --\n",
       "|    └─ReLU: 2-8                         --\n",
       "├─Flatten: 1-9                           --\n",
       "├─LogSoftmax: 1-10                       --\n",
       "=================================================================\n",
       "Total params: 0\n",
       "Trainable params: 0\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.KDleft1 = KDLeftLayer(input_dim=(28, 28), weight_dim=(28, 64), activation_string='ReLU')\n",
    "        self.KDright1 = KDRightLayer(input_dim=(28, 64), weight_dim=(64, 28), activation_string='ReLU')\n",
    "        \n",
    "        self.KDleft2 = KDLeftLayer(input_dim=(64, 64), weight_dim=(64, 64), activation_string='ReLU')\n",
    "        self.KDright2 = KDRightLayer(input_dim=(64, 64), weight_dim=(64, 64), activation_string='ReLU')\n",
    "        \n",
    "        \n",
    "        self.KDleft3 = KDLeftLayer(input_dim=(64, 64), weight_dim=(64, 64), activation_string='ReLU')\n",
    "        self.KDright3 = KDRightLayer(input_dim=(64, 64), weight_dim=(64, 64), activation_string='ReLU')\n",
    "        \n",
    "        \n",
    "        self.KDleft4 = KDLeftLayer(input_dim=(64, 64), weight_dim=(64, 5), activation_string='ReLU')\n",
    "        self.KDright4 = KDRightLayer(input_dim=(64, 5), weight_dim=(2, 64), activation_string='ReLU')\n",
    "        self.flat = nn.Flatten()\n",
    "        self.Softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.KDleft1(x)\n",
    "        x = self.KDright1(x)\n",
    "        \n",
    "        x = self.KDleft2(x)\n",
    "        x = self.KDright2(x)\n",
    "        \n",
    "        x = self.KDleft3(x)\n",
    "        x = self.KDright3(x)\n",
    "        \n",
    "        x = self.KDleft4(x)\n",
    "        x = self.KDright4(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.Softmax(x)\n",
    "        return x\n",
    "\n",
    "model = BasicModel()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ae7e1a1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b73f77c3",
    "outputId": "89d67ccf-33ca-411b-ab7a-9db3714b3202"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m     11\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     16\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/shapeworks/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[167], line 22\u001b[0m, in \u001b[0;36mBasicModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKDleft1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKDright1(x)\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKDleft2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/shapeworks/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[166], line 51\u001b[0m, in \u001b[0;36mKDLeftLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "loss_array = []\n",
    "# model = model.cuda()\n",
    "for e in range(100):\n",
    "    l = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.reshape(-1,28,28)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "\n",
    "        x = model(data)\n",
    "        x = x.reshape(-1,10)\n",
    "        target = target.reshape(-1,10)\n",
    "        print(x.shape)\n",
    "#         print(target.shape)\n",
    "#         loss = F.nll_loss(x,target).mean()\n",
    "        loss = bce_loss(x.float(),target.float()).sum()\n",
    "        l = l+loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    loss_array.append(l.detach().numpy())\n",
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d8f56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _report_model_parameters(model):\n",
    "        \"\"\" Reports the model size \"\"\"\n",
    "\n",
    "        all_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        size = all_params * (32 / 8)  # Bytes\n",
    "        print(all_params, trainable_params, size)\n",
    "#         print(\"Model has %.1f M parameters (%.1f M trainable) with an estimated size of %all_params, trainable_params / 1.0e6, size / 1.0e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fad7862b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 346 1384.0\n"
     ]
    }
   ],
   "source": [
    "_report_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd1c47dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f1834871040>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d9d9f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87fd9483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─KDRightLayer: 1-1                      --\n",
      "|    └─ReLU: 2-1                         --\n",
      "├─KDLeftLayer: 1-2                       --\n",
      "|    └─Identity: 2-2                     --\n",
      "├─Flatten: 1-3                           --\n",
      "├─LogSoftmax: 1-4                        --\n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─KDRightLayer: 1-1                      --\n",
       "|    └─ReLU: 2-1                         --\n",
       "├─KDLeftLayer: 1-2                       --\n",
       "|    └─Identity: 2-2                     --\n",
       "├─Flatten: 1-3                           --\n",
       "├─LogSoftmax: 1-4                        --\n",
       "=================================================================\n",
       "Total params: 0\n",
       "Trainable params: 0\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "887159e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "modelPoly = Polynomial3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94235eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2156.09521484375\n",
      "199 1431.120849609375\n",
      "299 951.0269775390625\n",
      "399 633.064453125\n",
      "499 422.4577331542969\n",
      "599 282.9432678222656\n",
      "699 190.5121612548828\n",
      "799 129.2667694091797\n",
      "899 88.6793212890625\n",
      "999 61.778175354003906\n",
      "1099 43.94529724121094\n",
      "1199 32.121734619140625\n",
      "1299 24.281173706054688\n",
      "1399 19.080839157104492\n",
      "1499 15.630996704101562\n",
      "1599 13.341946601867676\n",
      "1699 11.822671890258789\n",
      "1799 10.814119338989258\n",
      "1899 10.144418716430664\n",
      "1999 9.699589729309082\n",
      "Result: y = 0.009044463746249676 + 0.8290848731994629 x + -0.0015603243373334408 x^2 + -0.08939655870199203 x^3\n"
     ]
    }
   ],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined\n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(modelPoly.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = modelPoly(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {modelPoly.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12d41eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Polynomial3: 1-1                       4\n",
      "├─Polynomial3: 1-2                       4\n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Polynomial3: 1-1                       4\n",
       "├─Polynomial3: 1-2                       4\n",
       "=================================================================\n",
       "Total params: 8\n",
       "Trainable params: 8\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.linear = nn.Linear(256, 2)\n",
    "        self.p1 = Polynomial3()\n",
    "        self.p2 = Polynomial3()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.p1(x)\n",
    "        x = self.p2(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = BasicModel()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ce1b9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Polynomial3: 1-1                       4\n",
      "├─Polynomial3: 1-2                       4\n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Polynomial3: 1-1                       4\n",
       "├─Polynomial3: 1-2                       4\n",
       "=================================================================\n",
       "Total params: 8\n",
       "Trainable params: 8\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8e58e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 16.0\n"
     ]
    }
   ],
   "source": [
    "_report_model_parameters(modelPoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c39d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486795c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
